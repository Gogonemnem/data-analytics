{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TI3130: Classification Lab &mdash; Solutions (variant B)\n",
    "**Juli√°n Urbano &mdash; January 2022**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)] \n",
      "numpy 1.21.2 \n",
      "pandas 1.3.4 \n",
      "plotnine 0.8.0 \n",
      "statsmodels 0.13.0 \n",
      "sklearn 1.0.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from plotnine import __version__ as p9__version__\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import __version__ as sk__version__\n",
    "\n",
    "print(\"python\", sys.version,\n",
    "      \"\\nnumpy\", np.__version__,\n",
    "      \"\\npandas\", pd.__version__,\n",
    "      \"\\nplotnine\", p9__version__,\n",
    "      \"\\nstatsmodels\", sm.__version__,\n",
    "      \"\\nsklearn\", sk__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these exercises we will use the _Amsterdam Lite_ dataset and the _Heart_ dataset. Please refer to their HTML files for a description of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ams = pd.read_csv('amsterdam_lite.csv')\n",
    "for col in ams.select_dtypes('object').columns:\n",
    "    ams[col] = pd.Categorical(ams[col])\n",
    "\n",
    "heart = pd.read_csv('heart.csv')\n",
    "for col in heart.select_dtypes('object').columns:\n",
    "    heart[col] = pd.Categorical(heart[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these exercises we will use the evaluation metrics and cross-validation code we used in the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def kfold_cv(X, y, k, H, cv_fun, random_state):\n",
    "    \"\"\"\n",
    "    Do stratified k-fold cross-validation with a dataset, to check how a model behaves as a function\n",
    "    of the values in H (eg. a hyperparameter such as tree depth, or polynomial degree).\n",
    "\n",
    "    :param X: feature matrix.\n",
    "    :param y: response column.\n",
    "    :param k: number of folds.\n",
    "    :param H: values of the hyperparameter to cross-validate.\n",
    "    :param cv_fun: function of the form (X_train, y_train, X_valid, y_valid, h) to evaluate the model in one split,\n",
    "        as a function of h. It must return a dictionary with metric score values.\n",
    "    :param random_state: controls the pseudo random number generation for splitting the data.\n",
    "    :return: a Pandas dataframe with metric scores along values in H.\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits = k, shuffle = True, random_state = random_state)\n",
    "    pr = []  # to store global results\n",
    "\n",
    "    # for each value h in H, do CV\n",
    "    for h in H:\n",
    "        scores = []  # to store the k results for this h\n",
    "        # for each fold 1..K\n",
    "        for train_index, valid_index in kf.split(X, y):\n",
    "            # partition the data in training and validation\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "            # call cv_fun to train the model and compute performance\n",
    "            fold_scores = cv_fun(X_train, y_train, X_valid, y_valid, h)\n",
    "            scores.append(fold_scores)\n",
    "\n",
    "        rowMeans = pd.DataFrame(scores).mean(axis = 0)  # average scores across folds\n",
    "        pr.append(rowMeans)  # append to global results\n",
    "\n",
    "    pr = pd.DataFrame(pr).assign(_h = H)\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) The first classifier in the tutorial notebook used a logistic model with a default threshold of 0.5. Function `predict` below implements a prediction function for the given `model`, test data `X_test` and response `vocabulary`. Implement this function such that it optimizes the recall of the second class, `2.high`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample data and model\n",
    "X = ams.assign(y = ams['saf_catering'].cat.codes)\n",
    "m = smf.glm('y ~ hou_value', X, family = sm.families.Binomial()).fit()\n",
    "\n",
    "def predict(model, X_test, vocabulary):\n",
    "    p = model.predict(X_test)\n",
    "    # always predict 2.high\n",
    "    \n",
    "    return #TODO\n",
    "\n",
    "# sample execution\n",
    "predict(m, X, ams['saf_catering'].cat.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Slide 38 shows an example of how complex models keep improving in the training set but not necessarily in a validation set because they eventually overfit. Use the _Heart_ dataset and decision trees to predict `ahd` with _all_ other features to produce a plot similar to the one in the slides via 10-fold cross-validation. You only need to plot accuracy in the training and validation sets. What number of leaves would you choose based on the results? You will need to use Pandas' [`get_dummies`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to encode categorical features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Use Scikit-learn's [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to build a model to predict `saf_nonneighbors` using all integer features in the dataset (you may have to increase the `max_iter` argument to something like `1000`). Produce a full classification report.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Implement function `tree_ovo_fit` to fit a multi-class decision tree following a one-versus-one approach with individual binary decision trees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# sample data\n",
    "y = ams['spa_streets']\n",
    "X = pd.get_dummies(ams.filter(regex = '^(fac_|inc_|saf_)'))\n",
    "\n",
    "def tree_ovo_fit(X, y, random_state):\n",
    "    models = []\n",
    "    vocabulary = y.cat.categories    \n",
    "    \n",
    "    # for every pair of classes cl1-cl2\n",
    "        # filter out rows where the response is neither cl1 nor cl2\n",
    "        # fit a model cl1-vs-cl2\n",
    "        # store the model in models\n",
    "    #TODO\n",
    "            \n",
    "    return {'vocabulary': vocabulary,\n",
    "            'models': models}\n",
    "\n",
    "# sample execution\n",
    "m = tree_ovo_fit(X, y, random_state = 123)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Implement function `tree_ovo_predict` to predict from a model trained with `tree_ovo_fit`. The prediction should not be probabilistic (ie. it returns the predicted class). You will probably need a function to compute the mode (such as SciPy's [`mode`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html)), and a way to [flatten a list of lists](https://stackoverflow.com/a/953097/14674728).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "from itertools import chain\n",
    "\n",
    "# sample model\n",
    "m = tree_ovo_fit(X, y, random_state = 123)\n",
    "\n",
    "def tree_ovo_predict(model, X):\n",
    "    # prepare an array with as many rows as X, and as many columns as individual models\n",
    "    p = np.empty_like(model['vocabulary'], shape = [X.shape[0], len(model['models'])])\n",
    "    \n",
    "    # for every individual model\n",
    "        # make a prediction and store it in p\n",
    "    #TODO\n",
    "    \n",
    "    # select the classes that got predicted most often and return them\n",
    "    return #TODO\n",
    "\n",
    "p = tree_ovo_predict(m, X)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) Fit an SVM model to predict `district` based on *all other* features in the dataset, and produce a full classification report with the training data. Is there any problem with this model, and if so, what is the cause? You don't need to tune hyperparameters, but you will need to use Pandas' [`get_dummies`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to encode categorical features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g) Use 5-fold cross-validation to plot how the F-score is affected by `max_features` when building random forests to predict `ahd` based on _all_ other features in the _Heart_ dataset. Compare with the performance of a decision tree and a bagging model, also doing 5-fold cross-validation. Use 50 trees for random forests and bagging.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h) The tutorial notebook built a multinomial model to predict `spa_streets` based on some features and their interactions, but the model had clear issues of class imbalance whereby the majority class `2.average` dominated the learning process. The resulting macro-averaged F-score was indeed low at 0.39. Try solving this issue by fitting the same model on a new dataset where the minority classes `1.low` and `3.high` are oversampled to contain the same number of instances as the majority class. Compare the model with the one in the tutorial using the original dataset. Does the new model improve in terms of imbalance? Why? You can use Pandas' [`sample`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.sample.html) to oversample instances.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) Create an _unambiguous_ and _nontrivial_ question, and its corresponding solution, as if you were writing the set of exercises for the _Classification_ lab. The question must cover at least 3 of the following aspects:**\n",
    "\n",
    "- **Logistic or Multinomial regression for classification**\n",
    "- **Decision trees, bagging or random forests**\n",
    "- **Support Vector Machines**\n",
    "- **Model evaluation**\n",
    "- **Choice of hyperparameters via cross-validation**\n",
    "- **An open-ended question to explain some behavior**\n",
    "\n",
    "**Please make it explicit which 3 of these aspects your question covers. You can use any of the datasets available on Brightspace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
