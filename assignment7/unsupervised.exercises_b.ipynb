{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TI3130: Unsupervised Learning Lab &mdash; Exercises (variant B)\n",
    "**Juli√°n Urbano &mdash; January 2022**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)] \n",
      "numpy 1.21.2 \n",
      "pandas 1.3.5 \n",
      "plotnine 0.8.0 \n",
      "statsmodels 0.13.0 \n",
      "sklearn 0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from plotnine import __version__ as p9__version__\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import __version__ as sk__version__\n",
    "\n",
    "print(\"python\", sys.version,\n",
    "      \"\\nnumpy\", np.__version__,\n",
    "      \"\\npandas\", pd.__version__,\n",
    "      \"\\nplotnine\", p9__version__,\n",
    "      \"\\nstatsmodels\", sm.__version__,\n",
    "      \"\\nsklearn\", sk__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these exercises we will use the _Amsterdam Lite_ dataset, the _Heart_ dataset, and the _Arrests_ dataset about criminality in the US. Please refer to their HTML files for a description of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ams = pd.read_csv('amsterdam_lite.csv')\n",
    "for col in ams.select_dtypes('object').columns:\n",
    "    ams[col] = pd.Categorical(ams[col])\n",
    "\n",
    "heart = pd.read_csv('heart.csv')\n",
    "for col in heart.select_dtypes('object').columns:\n",
    "    heart[col] = pd.Categorical(heart[col])\n",
    "\n",
    "arrests = pd.read_csv('arrests.csv')\n",
    "for col in arrests.select_dtypes('object').columns:\n",
    "    arrests[col] = pd.Categorical(arrests[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these exercises we will use the evaluation metrics and cross-validation code we used in the _Classification_ tutorial, as well as all the visualization functions we used in the _Unsupervised Learning_ tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     "4"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def kfold_cv(X, y, k, H, cv_fun, random_state):\n",
    "    \"\"\"\n",
    "    Do stratified k-fold cross-validation with a dataset, to check how a model behaves as a function\n",
    "    of the values in H (eg. a hyperparameter such as tree depth, or polynomial degree).\n",
    "\n",
    "    :param X: feature matrix.\n",
    "    :param y: response column.\n",
    "    :param k: number of folds.\n",
    "    :param H: values of the hyperparameter to cross-validate.\n",
    "    :param cv_fun: function of the form (X_train, y_train, X_valid, y_valid, h) to evaluate the model in one split,\n",
    "        as a function of h. It must return a dictionary with metric score values.\n",
    "    :param random_state: controls the pseudo random number generation for splitting the data.\n",
    "    :return: a Pandas dataframe with metric scores along values in H.\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits = k, shuffle = True, random_state = random_state)\n",
    "    pr = []  # to store global results\n",
    "\n",
    "    # for each value h in H, do CV\n",
    "    for h in H:\n",
    "        scores = []  # to store the k results for this h\n",
    "        # for each fold 1..K\n",
    "        for train_index, valid_index in kf.split(X, y):\n",
    "            # partition the data in training and validation\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "            # call cv_fun to train the model and compute performance\n",
    "            fold_scores = cv_fun(X_train, y_train, X_valid, y_valid, h)\n",
    "            scores.append(fold_scores)\n",
    "\n",
    "        rowMeans = pd.DataFrame(scores).mean(axis = 0)  # average scores across folds\n",
    "        pr.append(rowMeans)  # append to global results\n",
    "\n",
    "    pr = pd.DataFrame(pr).assign(_h = H)\n",
    "    return pr\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def plot_dendrogram(model, labels, figsize=(14,6), **kwargs):\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    plt.figure(figsize=(14,6))\n",
    "    dendrogram(linkage_matrix, color_threshold = 0, leaf_rotation = 90, labels=labels, **kwargs)\n",
    "\n",
    "def pca_summary(pca_model):\n",
    "    sdev = np.sqrt(pca_model.explained_variance_)\n",
    "    var_cor = np.apply_along_axis(lambda c: c*sdev, 0, pca_model.components_)\n",
    "    var_cos2 = var_cor**2\n",
    "    comp_cos2 = np.sum(var_cos2, 1)\n",
    "    var_contrib = np.apply_along_axis(lambda c: c/comp_cos2, 0, var_cos2)\n",
    "    return var_cor, var_contrib\n",
    "\n",
    "def pca_screeplot(pca_model, figsize=(8,6)):\n",
    "    \"\"\"\n",
    "    Shows a barplot of the fraction of variance explained by each PC.\n",
    "    \"\"\"\n",
    "    var_cor, var_contrib = pca_summary(pca_model)\n",
    "    dim = np.arange(var_cor.shape[1])\n",
    "    var = pca_model.explained_variance_ratio_\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.rc('axes', axisbelow=True)\n",
    "    plt.grid(axis = 'y', linewidth = 0.5)\n",
    "    plt.bar(dim, 100*var)\n",
    "    plt.xticks(dim, dim+1)\n",
    "    plt.xlabel('Component')\n",
    "    plt.ylabel('Explained variance (%)')\n",
    "    plt.title('Scree plot')\n",
    "    plt.show()\n",
    "\n",
    "def pca_corplot(pca_model, column_names, comp = [0,1], figsize=(6,6)):\n",
    "    \"\"\"\n",
    "    Shows the variables in a correlation circle.\n",
    "    The projection of each variable on a PC represents its correlation with that PC.\n",
    "    \"\"\"\n",
    "    var_cor, var_contrib = pca_summary(pca_model)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.rc('axes', axisbelow=True)\n",
    "    plt.grid(linewidth = 0.5)\n",
    "    plt.axhline(linestyle='--', color='k')\n",
    "    plt.axvline(linestyle='--', color='k')\n",
    "    plt.gcf().gca().add_patch(plt.Circle((0,0),1,color='grey',fill=False))\n",
    "    plt.xlim([-1.1,1.1])\n",
    "    plt.ylim([-1.1,1.1])\n",
    "    plt.xlabel(f'PC{comp[0]+1}')\n",
    "    plt.ylabel(f'PC{comp[1]+1}')\n",
    "    plt.title(f'Correlation circle PC{comp[0]+1}-PC{comp[1]+1}')\n",
    "\n",
    "    for i in range(pca_model.n_components_):\n",
    "        x = var_cor[comp[0],i]\n",
    "        y = var_cor[comp[1],i]\n",
    "        plt.arrow(0,0,x,y, color='k',\n",
    "                 head_length=.025, head_width=.025, length_includes_head=True)\n",
    "        plt.text(x,y,\n",
    "                 horizontalalignment='left' if x>0 else 'right',\n",
    "                 verticalalignment='bottom' if y>0 else 'top',\n",
    "                 color='k', s=list(column_names)[i])\n",
    "    plt.show()\n",
    "\n",
    "def pca_contribplot(pca_model, column_names, comp = 0, figsize=(8,6)):\n",
    "    \"\"\"\n",
    "    Shows a barplot of the contribution of each variable to a PC.\n",
    "    \"\"\"\n",
    "    var_cor, var_contrib = pca_summary(pca_model)\n",
    "    dim = np.arange(var_cor.shape[1])\n",
    "    i = (-var_contrib[comp]).argsort()\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.rc('axes', axisbelow=True)\n",
    "    plt.grid(axis = 'y', linewidth = 0.5)\n",
    "    plt.bar(dim, 100*var_contrib[comp][i])\n",
    "    plt.axhline(y=100/len(dim), linestyle='--', color='r')\n",
    "    plt.xticks(dim, column_names[i])\n",
    "    plt.ylabel('Contribution (%)')\n",
    "    plt.title(f'Contribution of variables to PC{comp+1}')\n",
    "    plt.show()\n",
    "\n",
    "def pca_biplot(X, pca_model, column_names, comp = [0,1], clusters = None, labels = None, figsize=(12,10)):\n",
    "    \"\"\"\n",
    "    Shows a scatterplot showing each observation in the 2D space defined by two PCs,\n",
    "    along with the vectors corresponding to each feature.\n",
    "    Observations can optionally be identified by a label and a color (eg. to identify clusters).\n",
    "    Adapted from https://jbhender.github.io/Stats506/F17/Projects/G18.html\n",
    "    \"\"\"\n",
    "    xvector = pca_model.components_[comp[0]]\n",
    "    yvector = pca_model.components_[comp[1]]\n",
    "\n",
    "    xs = pca_model.transform(X)[:,comp[0]]\n",
    "    ys = pca_model.transform(X)[:,comp[1]]\n",
    "        \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.rc('axes', axisbelow=True)\n",
    "    plt.grid(linewidth = 0.5)\n",
    "    plt.axhline(linestyle='--', color='k')\n",
    "    plt.axvline(linestyle='--', color='k')\n",
    "    plt.xlim([min([0, min(xs), min(xvector*max(xs)*1.1)]),\n",
    "              max([0, max(xs), max(xvector*max(xs)*1.1)])\n",
    "             ])\n",
    "    plt.ylim([min([0, min(ys), min(yvector*max(ys)*1.1)]),\n",
    "              max([0, max(ys), max(yvector*max(ys)*1.1)])\n",
    "             ])\n",
    "    plt.xlabel(f'PC{comp[0]+1}')\n",
    "    plt.ylabel(f'PC{comp[1]+1}')\n",
    "    plt.title(f'Biplot PC{comp[0]+1}-PC{comp[1]+1}')\n",
    "    \n",
    "    # plot vectors\n",
    "    for i in range(len(xvector)):\n",
    "        x = xvector[i]*max(xs)\n",
    "        y = yvector[i]*max(ys)\n",
    "        plt.arrow(0, 0, x, y, color='k',\n",
    "                 head_length=.1, head_width=.1, length_includes_head=True)\n",
    "        plt.text(x*1.1, y*1.1, color='k', s=list(column_names)[i])\n",
    "\n",
    "    # plot observations+\n",
    "    colors = cm.get_cmap('tab10').colors\n",
    "    if clusters is None:\n",
    "        clusters = np.zeros(X.shape[0], dtype=int)\n",
    "    if labels is None:\n",
    "        labels = np.arange(X.shape[0]).astype('str')\n",
    "    for i in range(len(set(clusters))):\n",
    "        plt.plot(0,0, color = colors[i], label=str(i)) # only to make sure the legend appears\n",
    "    plt.legend()\n",
    "    for i in range(len(xs)):\n",
    "        plt.text(xs[i], ys[i], color = colors[clusters[i]], label=str(clusters[i]), s = labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Train a PCA model on variables `murder`, `rape`, `assault` and `urban_pop` from the _Arrests_ dataset. Do not forget to standardize the data with `StandardScaler`. Visualize the model with function `pca_biplot`. Which appear to be the 3 most dangerous states?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Do the same as in a), but use rates per population instead of counts (ie. `murder/population` instead of `murder`). Visualize the model and find out the 3 most dangerous states. Why are the results different? We standardized the dataset with rates prior to running PCA. Why doesn't that solve the problem?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Run 4-means clustering on all numeric variables of the _Heart_ dataset. Fit a decision tree to the same data to predict the clusters, so that we may try and interpret them. What 2 variables seem to be the most important in defining the clusters? You don't need to create an image of the tree; you can simply use [`tree.export_text`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html) (refer back to the _Classification_ tutorial for reference).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Use PCA to try and find what numerical variables in the _Amsterdam_ dataset seem to be promising for the purposes of predicting `spa_streets`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Create an _unambiguous_ and _nontrivial_ question, and its corresponding solution, as if you were writing the set of exercises for the _Unsupervised Learning_ lab. The question must cover at least 3 of the following aspects:**\n",
    "\n",
    "- **K-means, hierarchical clustering or PCA**\n",
    "- **Interpretation or visualization of clusters or principal components**\n",
    "- **Choice of hyperparameters**\n",
    "- **Unsupervised learning to aid in the construction of supervised models**\n",
    "- **An open-ended question to explain some behavior**\n",
    "\n",
    "**Please make it explicit which 3 of these aspects your question covers. You can use any of the datasets available on Brightspace.**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
